{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674ec1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "import ltn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b94b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed88547",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de01e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(idx, embedding_size=151):\n",
    "    t = [0] * embedding_size\n",
    "    t[idx] = 1\n",
    "    t = torch.tensor(t)\n",
    "    return t.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c5d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGImageDataset(Dataset):\n",
    "    def __init__(self, imdb_h5, sgg_h5, sgg_dict):\n",
    "        self.imdb = h5py.File(imdb_h5)\n",
    "        self.sgg = h5py.File(sgg_h5)\n",
    "        with open(sgg_dict) as f:\n",
    "            self.dicts = json.load(f)\n",
    "            self.idx_to_labels = self.dicts[\"idx_to_label\"]\n",
    "            self.label_to_idx = self.dicts[\"label_to_idx\"]\n",
    "            self.idx_to_predicates = self.dicts[\"idx_to_predicate\"]\n",
    "            self.predicates_to_idx = self.dicts[\"predicate_to_idx\"]\n",
    "\n",
    "        def return_set():\n",
    "            return set()\n",
    "\n",
    "        self.logic_rules = defaultdict(return_set)\n",
    "\n",
    "        for i in range(len(self.sgg[\"relationships\"])):\n",
    "            sub, obj, rel = (\n",
    "                self.sgg[\"relationships\"][i][0],\n",
    "                self.sgg[\"relationships\"][i][1],\n",
    "                self.sgg[\"predicates\"][i],\n",
    "            )\n",
    "            self.logic_rules[\n",
    "                (\n",
    "                    self.idx_to_labels[str(self.sgg[\"labels\"][sub][0])].upper(),\n",
    "                    self.idx_to_predicates[str(rel[0])].upper().replace(\" \", \"_\"),\n",
    "                )\n",
    "            ].add(self.idx_to_labels[str(self.sgg[\"labels\"][obj][0])].upper())\n",
    "\n",
    "        self.g = {label: get_tensor(i) for i, label in enumerate(self.label_to_idx)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imdb[\"images\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = torch.tensor(self.imdb[\"images\"][idx])\n",
    "\n",
    "        start = self.sgg[\"img_to_first_box\"][idx]\n",
    "        end = self.sgg[\"img_to_last_box\"][idx]\n",
    "        object_names = []\n",
    "        if start > 0:\n",
    "            object_names = [self.sgg[\"labels\"][i][0] for i in range(start, end + 1)]\n",
    "        for _ in range(len(object_names), 150):\n",
    "            object_names += [150]\n",
    "\n",
    "        return image, object_names\n",
    "\n",
    "    def get_relations_grounding(self):\n",
    "        dic = {\n",
    "            predicate.upper().replace(\" \", \"_\"): ltn.Constant(get_tensor(idx))\n",
    "            for idx, predicate in enumerate(self.predicates_to_idx)\n",
    "        }\n",
    "        return dic\n",
    "\n",
    "    def get_logic_rules(self):\n",
    "        return self.logic_rules\n",
    "\n",
    "    def get_object_grounding(self):\n",
    "        dic = {\n",
    "            obj.upper(): ltn.Constant(get_tensor(idx))\n",
    "            for idx, obj in enumerate(self.label_to_idx)\n",
    "        }\n",
    "        return dic\n",
    "\n",
    "    def colate_fn(self, data):\n",
    "        batch = [d for d in data if d != None]\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5363b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects(image):\n",
    "    yolo = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n",
    "    yolo.eval()\n",
    "    with torch.no_grad():\n",
    "        output = yolo(image)\n",
    "\n",
    "    objects = []\n",
    "    for *_, confidence, classification in output.pred[0]:\n",
    "        objects.append((output.names[int(classification.item())], confidence.item()))\n",
    "\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbd3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgg_path = \"../data/mini_VG-SGG.h5\"\n",
    "sgg_dict_path = \"../data/mini_VG-SGG-dicts.json\"\n",
    "imdb_path = \"../data/mini_imdb_1024.h5\"\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "train_data = VGImageDataset(imdb_path, sgg_path, sgg_dict_path)\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, BATCH_SIZE, shuffle=True, collate_fn=train_data.colate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b57a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define connectives, quantifiers, and SatAgg\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0bae7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicate(model=MLP(\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (linear_layers): ModuleList(\n",
       "    (0): Linear(in_features=302, out_features=250, bias=True)\n",
       "    (1): Linear(in_features=250, out_features=200, bias=True)\n",
       "    (2): Linear(in_features=200, out_features=151, bias=True)\n",
       "  )\n",
       "  (softmax): Softmax(dim=1)\n",
       "))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes=(302, 250, 200, 151)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.linear_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                for i in range(1, len(layer_sizes))\n",
    "            ]\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(1)\n",
    "\n",
    "    def forward(self, l, *x):\n",
    "        x = list(x)\n",
    "        if len(x) == 1:\n",
    "            x = x[0]\n",
    "        else:\n",
    "            x = torch.cat(x, dim=1)\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "relation_predicate = ltn.Predicate(MLP())\n",
    "relation_predicate.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d175702",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_grounding = train_data.get_relations_grounding()\n",
    "obj_grounding = train_data.get_object_grounding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ddbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(relation_predicate.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23e020d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/abhiram/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-9-27 Python-3.9.18 torch-2.0.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Predicate(model=LambdaModel())"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement object predicate.\n",
    "def return_zero():\n",
    "    return 0\n",
    "\n",
    "\n",
    "class Saved_Model:\n",
    "    def __init__(self) -> None:\n",
    "        self.yolo = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\", pretrained=True)\n",
    "        self.yolo.eval()\n",
    "        self.objects = None\n",
    "\n",
    "    def extract_objects(self, image):\n",
    "        image = image.permute(1, 2, 0)\n",
    "        image = image.numpy()\n",
    "        with torch.no_grad():\n",
    "            output = self.yolo([image])\n",
    "\n",
    "        objects = defaultdict(return_zero)\n",
    "        for *_, confidence, classification in output.pred[0]:\n",
    "            if (\n",
    "                obj_grounding.get(output.names[int(classification.item())].upper(), -1)\n",
    "                == -1\n",
    "            ):\n",
    "                continue\n",
    "            else:\n",
    "                objects[\n",
    "                    obj_grounding[\n",
    "                        output.names[int(classification.item())].upper()\n",
    "                    ].value\n",
    "                ] = max(\n",
    "                    objects[\n",
    "                        obj_grounding[\n",
    "                            output.names[int(classification.item())].upper()\n",
    "                        ].value\n",
    "                    ],\n",
    "                    confidence.item(),\n",
    "                )\n",
    "            # objects.append((output.names[int(classification.item())], confidence.item()))\n",
    "\n",
    "        self.objects = objects\n",
    "        return objects\n",
    "\n",
    "    def get_prob(self, x, labels):\n",
    "        res = []\n",
    "        for i in range(len(x)):\n",
    "            objects = self.extract_objects(x[i])\n",
    "            probs = []\n",
    "            for l in range(labels.shape[0] // x.shape[0]):\n",
    "                probs.append(objects[labels[l]])\n",
    "            res.append(probs)\n",
    "        res = torch.tensor(res)\n",
    "        return res\n",
    "        # return torch.tensor(\n",
    "        #     [[1.0 for _ in range(labels.shape[0] // BATCH_SIZE)] for _ in range(128)]\n",
    "        # )\n",
    "\n",
    "\n",
    "model = Saved_Model()\n",
    "object_predicate = ltn.Predicate(func=lambda a, b: model.get_prob(a, b))\n",
    "object_predicate.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d4a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logic rule:  0\n",
      "logic rule:  1\n",
      "logic rule:  2\n",
      "logic rule:  3\n",
      "logic rule:  4\n",
      "logic rule:  5\n",
      "logic rule:  6\n",
      "logic rule:  7\n",
      "logic rule:  8\n",
      "logic rule:  9\n",
      "logic rule:  10\n",
      "logic rule:  11\n",
      "logic rule:  12\n",
      "logic rule:  13\n",
      "logic rule:  14\n",
      "logic rule:  15\n",
      "logic rule:  16\n",
      "logic rule:  17\n",
      "logic rule:  18\n",
      "logic rule:  19\n",
      "logic rule:  20\n",
      "logic rule:  21\n",
      "logic rule:  22\n",
      "logic rule:  23\n",
      "logic rule:  24\n",
      "logic rule:  25\n",
      "logic rule:  26\n",
      "logic rule:  27\n",
      "logic rule:  28\n",
      "logic rule:  29\n",
      "logic rule:  30\n",
      "logic rule:  31\n",
      "logic rule:  32\n",
      "logic rule:  33\n",
      "logic rule:  34\n",
      "logic rule:  35\n",
      "logic rule:  36\n",
      "logic rule:  37\n",
      "logic rule:  38\n",
      "logic rule:  39\n",
      "logic rule:  40\n",
      "logic rule:  41\n",
      "logic rule:  42\n",
      "logic rule:  43\n",
      "logic rule:  44\n",
      "logic rule:  45\n",
      "logic rule:  46\n",
      "logic rule:  47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m z \u001b[39m=\u001b[39m ltn\u001b[39m.\u001b[39mVariable(\u001b[39m\"\u001b[39m\u001b[39mz\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mstack([sub\u001b[39m.\u001b[39mvalue]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# print(Forall([x, z], object_predicate(x, z)).value)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# x = []\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# for img in images:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#         y.append((img, obj_grounding[idx].value))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# y = ltn.Variable(\"y\", torch.tensor(y))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m axioms \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     Forall(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         [z, y],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         Forall(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m             x,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m             Implies(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m                 And(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                     object_predicate(x, z),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                     relation_predicate(rel, y, z),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                 ),\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                 object_predicate(x, y),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlogic rule: \u001b[39m\u001b[39m\"\u001b[39m, i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/ltn/core.py:613\u001b[0m, in \u001b[0;36mPredicate.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m proc_objs, output_vars, output_shape \u001b[39m=\u001b[39m process_ltn_objects(inputs)\n\u001b[1;32m    612\u001b[0m \u001b[39m# the management of the input is left to the model or the lambda function\u001b[39;00m\n\u001b[0;32m--> 613\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m[o\u001b[39m.\u001b[39;49mvalue \u001b[39mfor\u001b[39;49;00m o \u001b[39min\u001b[39;49;00m proc_objs], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    615\u001b[0m \u001b[39m# check if output of predicate contains only truth values, namely values in the range [0., 1.]\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mall(torch\u001b[39m.\u001b[39mwhere(torch\u001b[39m.\u001b[39mlogical_and(output \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m, output \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m), \u001b[39m1.\u001b[39m, \u001b[39m0.\u001b[39m)):\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/ltn/core.py:356\u001b[0m, in \u001b[0;36mLambdaModel.forward\u001b[0;34m(self, *x)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mx):\n\u001b[0;32m--> 356\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49mx)\n",
      "\u001b[1;32m/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb Cell 13\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         \u001b[39m# return torch.tensor(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m         \u001b[39m#     [[1.0 for _ in range(labels.shape[0] // BATCH_SIZE)] for _ in range(128)]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m         \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m model \u001b[39m=\u001b[39m Saved_Model()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m object_predicate \u001b[39m=\u001b[39m ltn\u001b[39m.\u001b[39mPredicate(func\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m a, b: model\u001b[39m.\u001b[39;49mget_prob(a, b))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m object_predicate\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     objects \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_objects(x[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     probs \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(labels\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n",
      "\u001b[1;32m/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49myolo([image])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m objects \u001b[39m=\u001b[39m defaultdict(return_zero)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39m*\u001b[39m_, confidence, classification \u001b[39min\u001b[39;00m output\u001b[39m.\u001b[39mpred[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:717\u001b[0m, in \u001b[0;36mAutoShape.forward\u001b[0;34m(self, ims, size, augment, profile)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[39mwith\u001b[39;00m amp\u001b[39m.\u001b[39mautocast(autocast):\n\u001b[1;32m    715\u001b[0m     \u001b[39m# Inference\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39mwith\u001b[39;00m dt[\u001b[39m1\u001b[39m]:\n\u001b[0;32m--> 717\u001b[0m         y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x, augment\u001b[39m=\u001b[39;49maugment)  \u001b[39m# forward\u001b[39;00m\n\u001b[1;32m    719\u001b[0m     \u001b[39m# Post-process\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39mwith\u001b[39;00m dt[\u001b[39m2\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:527\u001b[0m, in \u001b[0;36mDetectMultiBackend.forward\u001b[0;34m(self, im, augment, visualize)\u001b[0m\n\u001b[1;32m    524\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)  \u001b[39m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpt:  \u001b[39m# PyTorch\u001b[39;00m\n\u001b[0;32m--> 527\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im, augment\u001b[39m=\u001b[39maugment, visualize\u001b[39m=\u001b[39mvisualize) \u001b[39mif\u001b[39;00m augment \u001b[39mor\u001b[39;00m visualize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(im)\n\u001b[1;32m    528\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit:  \u001b[39m# TorchScript\u001b[39;00m\n\u001b[1;32m    529\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(im)\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:209\u001b[0m, in \u001b[0;36mDetectionModel.forward\u001b[0;34m(self, x, augment, profile, visualize)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m augment:\n\u001b[1;32m    208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_augment(x)  \u001b[39m# augmented inference, None\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_once(x, profile, visualize)\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/yolo.py:121\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mif\u001b[39;00m profile:\n\u001b[1;32m    120\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 121\u001b[0m x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[1;32m    122\u001b[0m y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/torch/hub/ultralytics_yolov5_master/models/common.py:245\u001b[0m, in \u001b[0;36mSPPF.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    243\u001b[0m y1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm(x)\n\u001b[1;32m    244\u001b[0m y2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mm(y1)\n\u001b[0;32m--> 245\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv2(torch\u001b[39m.\u001b[39mcat((x, y1, y2, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mm(y2)), \u001b[39m1\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/_jit_internal.py:484\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    483\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        images = data[0]\n",
    "        logic_rules = train_data.get_logic_rules()\n",
    "        axioms = []\n",
    "        i = 0\n",
    "        for k in logic_rules:\n",
    "            sub = obj_grounding[k[0]]\n",
    "            rel = relation_grounding[k[1]]\n",
    "            objs = logic_rules[k]\n",
    "            y = ltn.Variable(\n",
    "                \"y\", torch.stack([obj_grounding[idx].value for idx in objs])\n",
    "            )\n",
    "            x = ltn.Variable(\"x\", images)\n",
    "            z = ltn.Variable(\"z\", torch.stack([sub.value]))\n",
    "            # print(Forall([x, z], object_predicate(x, z)).value)\n",
    "            # x = []\n",
    "            # for img in images:\n",
    "            #     x.append([img, sub.value])\n",
    "            #     # print(img.shape, sub.value.shape)\n",
    "            # x = ltn.Variable('x', torch.tensor(x))\n",
    "            # y = []\n",
    "            # for obj in objs:\n",
    "            #     for img in images:\n",
    "            #         y.append((img, obj_grounding[idx].value))\n",
    "            # y = ltn.Variable(\"y\", torch.tensor(y))\n",
    "            axioms += [\n",
    "                Forall(\n",
    "                    [z, y],\n",
    "                    Forall(\n",
    "                        x,\n",
    "                        Implies(\n",
    "                            And(\n",
    "                                object_predicate(x, z),\n",
    "                                relation_predicate(rel, y, z),\n",
    "                            ),\n",
    "                            object_predicate(x, y),\n",
    "                        ),\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "            print(\"logic rule: \", i)\n",
    "            i += 1\n",
    "        print(sat_agg)\n",
    "        sat_agg = SatAgg(*axioms)\n",
    "        loss = 1.0 - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(\" epoch %d | loss %.4f | Train Sat %.3f \" % (epoch, loss, sat_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fad447",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"./output/relation_predicate.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c109fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image\n",
    "\n",
    "img = image.imread(\"images.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "696a9c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/abhiram/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-9-27 Python-3.9.18 torch-2.0.1 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "objects = extract_objects(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "166a2351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person', 0.7376260161399841),\n",
       " ('bicycle', 0.7286328077316284),\n",
       " ('person', 0.6313160061836243),\n",
       " ('person', 0.6134570240974426),\n",
       " ('bicycle', 0.5618255734443665),\n",
       " ('bus', 0.4590524435043335)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1bdb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = ltn.Constant(torch.tensor(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8e6ea8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = obj_grounding[\"PERSON\"]\n",
    "rel = relation_grounding[\"made_of\".upper()]\n",
    "obj = obj_grounding[\"BUS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c455503",
   "metadata": {},
   "outputs": [],
   "source": [
    "Implies(And(object_predicate(img, sub), relation_predicate(rel, sub, obj)), object_predicate(img, obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f9555f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 255, 197, 3])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 1])\n",
      "2\n",
      "torch.Size([1, 255, 197, 3])\n",
      "torch.Size([1, 151])\n",
      "torch.Size([1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.99980, grad_fn=<RsubBackward1>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SatAgg(Implies(And(object_predicate(img_tensor, sub), relation_predicate(rel, sub, obj)), object_predicate(img_tensor, obj)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4080778c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
