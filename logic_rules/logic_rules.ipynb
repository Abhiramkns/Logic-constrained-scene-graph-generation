{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "674ec1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 21:57:40.414602: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 21:57:40.788619: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-27 21:57:40.925917: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-09-27 21:57:41.939527: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-27 21:57:41.939827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-09-27 21:57:41.939838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import json\n",
    "import ltn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b94b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed88547",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de01e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensor(idx, embedding_size=151):\n",
    "    t = [0] * embedding_size\n",
    "    t[idx] = 1\n",
    "    t = torch.tensor(t)\n",
    "    return t.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c5d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, imdb_h5, sgg_h5, sgg_dict, transform=None, target_transform=None\n",
    "    ):\n",
    "        self.imdb = h5py.File(imdb_h5)\n",
    "        self.sgg = h5py.File(sgg_h5)\n",
    "        with open(sgg_dict) as f:\n",
    "            self.dicts = json.load(f)\n",
    "            self.idx_to_labels = self.dicts[\"idx_to_label\"]\n",
    "            self.label_to_idx = self.dicts[\"label_to_idx\"]\n",
    "            self.idx_to_predicates = self.dicts[\"idx_to_predicate\"]\n",
    "            self.predicates_to_idx = self.dicts[\"predicate_to_idx\"]\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        def return_set():\n",
    "            return set()\n",
    "\n",
    "        self.logic_rules = defaultdict(return_set)\n",
    "\n",
    "        for i in range(len(self.sgg[\"relationships\"])):\n",
    "            sub, obj, rel = (\n",
    "                self.sgg[\"relationships\"][i][0],\n",
    "                self.sgg[\"relationships\"][i][1],\n",
    "                self.sgg[\"predicates\"][i],\n",
    "            )\n",
    "            self.logic_rules[\n",
    "                (\n",
    "                    self.idx_to_labels[str(self.sgg[\"labels\"][sub][0])].upper(),\n",
    "                    self.idx_to_predicates[str(rel[0])].upper().replace(\" \", \"_\"),\n",
    "                )\n",
    "            ].add(self.idx_to_labels[str(self.sgg[\"labels\"][obj][0])].upper())\n",
    "\n",
    "        self.g = {label: get_tensor(i) for i, label in enumerate(self.label_to_idx)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imdb[\"images\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.imdb[\"images\"][idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(torch.tensor(image))\n",
    "        image.to(torch.device(device))\n",
    "\n",
    "        start = self.sgg[\"img_to_first_box\"][idx]\n",
    "        end = self.sgg[\"img_to_last_box\"][idx]\n",
    "        object_names = []\n",
    "        if start > 0:\n",
    "            object_names = [self.sgg[\"labels\"][i][0] for i in range(start, end + 1)]\n",
    "        for _ in range(len(object_names), 150):\n",
    "            object_names += [150]\n",
    "\n",
    "        return image, object_names\n",
    "\n",
    "    def get_relations_grounding(self):\n",
    "        dic = {\n",
    "            predicate.upper().replace(\" \", \"_\"): ltn.Constant(get_tensor(idx))\n",
    "            for idx, predicate in enumerate(self.predicates_to_idx)\n",
    "        }\n",
    "        return dic\n",
    "\n",
    "    def get_logic_rules(self):\n",
    "        return self.logic_rules\n",
    "\n",
    "    def get_object_grounding(self):\n",
    "        dic = {\n",
    "            obj.upper(): ltn.Constant(get_tensor(idx))\n",
    "            for idx, obj in enumerate(self.label_to_idx)\n",
    "        }\n",
    "        return dic\n",
    "\n",
    "    def colate_fn(self, data):\n",
    "        batch = [d for d in data if d != None]\n",
    "        return torch.utils.data.dataloader.default_collate(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5363b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/grav/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-9-25 Python-3.10.12 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.1M/14.1M [00:09<00:00, 1.62MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolo = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "yolo.eval()\n",
    "\n",
    "def extract_objects(image):\n",
    "    with torch.no_grad():\n",
    "        output = yolo(image)\n",
    "    \n",
    "    objects = []\n",
    "    for *_, confidence, classification in output.pred[0]:\n",
    "        objects.append((output.names[int(classification.item())], confidence.item()))\n",
    "    \n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9d3f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement object predicate.\n",
    "\n",
    "\n",
    "class Saved_Model:\n",
    "    def __init__(self, path) -> None:\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        resnet = resnet50(weights=weights)\n",
    "        resnet.fc = torch.nn.Linear(resnet.fc.in_features, 40)\n",
    "        # self.model = resnet.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    def get_prob(self, x, label):\n",
    "        # prediction = self.model(x)\n",
    "        \n",
    "        return torch.tensor([[1.0 for _ in range(label.shape[0]//BATCH_SIZE)] for _ in range(128)]) # TODO: Get probability of a class\n",
    "\n",
    "\n",
    "saved_model_path = \"\"\n",
    "model = Saved_Model(path=saved_model_path)\n",
    "object_predicate = ltn.Predicate(func=lambda a, b: model.get_prob(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cbd3f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"<saved_model_directory>\"\n",
    "sgg_path = \"../data/mini_VG-SGG.h5\"\n",
    "sgg_dict_path = \"../data/mini_VG-SGG-dicts.json\"\n",
    "imdb_path = \"../data/mini_imdb_1024.h5\"\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "train_data = VGImageDataset(\n",
    "    imdb_path, sgg_path, sgg_dict_path, transform=weights.transforms()\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, BATCH_SIZE, shuffle=True, collate_fn=train_data.colate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b57a187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define connectives, quantifiers, and SatAgg\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0bae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, layer_sizes=(302, 250, 200, 151)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.linear_layers = torch.nn.ModuleList(\n",
    "            [\n",
    "                torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                for i in range(1, len(layer_sizes))\n",
    "            ]\n",
    "        )\n",
    "        self.softmax = torch.nn.Softmax(1)\n",
    "\n",
    "    def forward(self, l, *x):\n",
    "        x = list(x)\n",
    "        if len(x) == 1:\n",
    "            x = x[0]\n",
    "        else:\n",
    "            x = torch.cat(x, dim=1)\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "relation_predicate = ltn.Predicate(MLP())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d175702",
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_grounding = train_data.get_relations_grounding()\n",
    "obj_grounding = train_data.get_object_grounding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6ddbfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(relation_predicate.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91d4a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhiram/anaconda3/envs/prproject/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     y \u001b[39m=\u001b[39m ltn\u001b[39m.\u001b[39mVariable(\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m, torch\u001b[39m.\u001b[39mstack([idx\u001b[39m.\u001b[39mvalue \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m y]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m ltn\u001b[39m.\u001b[39mVariable(\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m, images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     axioms \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         Forall(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m             [x, y],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m             Implies(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m                 And(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     object_predicate(x, sub),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                     relation_predicate(rel, sub, y),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                 ),\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                 object_predicate(x, y),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m             ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m sat_agg \u001b[39m=\u001b[39m SatAgg(\u001b[39m*\u001b[39maxioms)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/abhiram/PRProject/PRProject/logic_rules/logic_rules.ipynb#Y165sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m sat_agg\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/ltn/core.py:610\u001b[0m, in \u001b[0;36mPredicate.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(x, LTNObject) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m inputs):\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39minputs\u001b[39m\u001b[39m'\u001b[39m\u001b[39m to be a tuple of LTNObject, but got \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m([\u001b[39mtype\u001b[39m(i)\n\u001b[1;32m    608\u001b[0m                                                                                               \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m inputs]))\n\u001b[0;32m--> 610\u001b[0m proc_objs, output_vars, output_shape \u001b[39m=\u001b[39m process_ltn_objects(inputs)\n\u001b[1;32m    612\u001b[0m \u001b[39m# the management of the input is left to the model or the lambda function\u001b[39;00m\n\u001b[1;32m    613\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m[o\u001b[39m.\u001b[39mvalue \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m proc_objs], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/prproject/lib/python3.9/site-packages/ltn/core.py:315\u001b[0m, in \u001b[0;36mprocess_ltn_objects\u001b[0;34m(objects)\u001b[0m\n\u001b[1;32m    313\u001b[0m     o\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munsqueeze(o\u001b[39m.\u001b[39mvalue, dim\u001b[39m=\u001b[39mnew_var_idx)\n\u001b[1;32m    314\u001b[0m     \u001b[39m# repeat existing dims along the new dim related to the new variable that has to be added to the object\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     o\u001b[39m.\u001b[39mvalue \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrepeat_interleave(o\u001b[39m.\u001b[39;49mvalue, repeats\u001b[39m=\u001b[39;49mvars_to_n[new_var], dim\u001b[39m=\u001b[39;49mnew_var_idx)\n\u001b[1;32m    316\u001b[0m     vars_in_obj\u001b[39m.\u001b[39mappend(new_var)\n\u001b[1;32m    318\u001b[0m \u001b[39m# permute the dimensions of the object in such a way the shapes of the processed objects is the same\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[39m# the shape is computed based on the order in which the variables are found at the beginning of this function\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        images = data[0]\n",
    "        logic_rules = train_data.get_logic_rules()\n",
    "        axioms = []\n",
    "        for k in logic_rules:\n",
    "            sub = obj_grounding[k[0]]\n",
    "            rel = relation_grounding[k[1]]\n",
    "            objs = logic_rules[k]\n",
    "            y = [obj_grounding[idx] for idx in objs]\n",
    "            y = ltn.Variable('y', torch.stack([idx.value for idx in y]))\n",
    "            x = ltn.Variable('x', images)\n",
    "            axioms += [\n",
    "                Forall(\n",
    "                    [x, y],\n",
    "                    Implies(\n",
    "                        And(\n",
    "                            object_predicate(x, sub),\n",
    "                            relation_predicate(rel, sub, y),\n",
    "                        ),\n",
    "                        object_predicate(x, y),\n",
    "                    ),\n",
    "                )\n",
    "            ]\n",
    "        sat_agg = SatAgg(*axioms)\n",
    "        loss = 1.0 - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(\" epoch %d | loss %.4f | Train Sat %.3f \" % (epoch, loss, sat_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914b24fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
